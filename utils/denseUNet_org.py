"def flow_layer(input_=None):\n",
    "    shape = input_.get_shape().as_list()\n",
    "    filters = shape[-1]\n",
    "    c = tf.layers.conv2d(input_, filters, 3, padding='SAME', activation=tf.nn.leaky_relu, kernel_initializer=tf.keras.initializers.he_normal())\n",
    "    c = tf.layers.conv2d(c, filters, 3, padding='SAME', activation=tf.nn.leaky_relu, kernel_initializer=tf.keras.initializers.he_normal())\n",
    "    c = tf.layers.conv2d(c, (ANGULAR_RES_TARGET)*2, 1, padding='SAME', activation=None, kernel_initializer=tf.keras.initializers.he_normal())\n",
    "    return c\n",
    "\n",
    "def conv(input_, filters):\n",
    "    c = tf.layers.conv2d(input_, filters, 3, padding='SAME', activation=tf.nn.leaky_relu, kernel_initializer=tf.keras.initializers.he_normal())\n",
    "    return c\n",
    "\n",
    "def conv_final(input_, filters):\n",
    "    c = tf.layers.conv2d(input_, filters, 3, padding='SAME', activation=tf.nn.tanh, kernel_initializer=tf.keras.initializers.he_normal())\n",
    "    return c\n",
    "\n",
    "def conv_conv(input_, filters):\n",
    "    c = tf.layers.conv2d(input_, filters, 3, padding='SAME', activation=tf.nn.leaky_relu, kernel_initializer=tf.keras.initializers.he_normal())\n",
    "    c = tf.layers.conv2d(c, filters, 3, padding='SAME', activation=None, kernel_initializer=tf.keras.initializers.he_normal())\n",
    "    #c = tf.layers.batch_normalization(c, training=TRAINING)\n",
    "    c = tf.nn.leaky_relu(c)\n",
    "    return c\n",
    "\n",
    "def downsample(input_, filters):\n",
    "    c = tf.layers.conv2d(input_, filters, 3, 2, padding='SAME', activation=tf.nn.leaky_relu, kernel_initializer=tf.keras.initializers.he_normal())\n",
    "    return c\n",
    "\n",
    "def upsample(input_, filters):\n",
    "    c = tf.layers.conv2d_transpose(input_, filters, 3, 2, padding='SAME', activation=tf.nn.leaky_relu, kernel_initializer=tf.keras.initializers.he_normal())\n",
    "    return c\n",
    "\n",
    "def upsample_concat(input_, input2_, filters):\n",
    "    c = upsample(input_, filters)\n",
    "    u = tf.concat((c, input2_), -1)\n",
    "    return u\n",
    "\n",
    "def upsample_conv_conv(input_, filters):\n",
    "    c = tf.layers.conv2d_transpose(input_, filters, 3, 2, padding='SAME', activation=tf.nn.leaky_relu, kernel_initializer=tf.keras.initializers.he_normal())\n",
    "    c = tf.layers.conv2d(c, filters, 3, padding='SAME', activation=tf.nn.leaky_relu, kernel_initializer=tf.keras.initializers.he_normal())\n",
    "    c = tf.layers.conv2d(c, filters, 3, padding='SAME', activation=tf.nn.tanh, kernel_initializer=tf.keras.initializers.he_normal())\n",
    "    return c\n",
    "\n",
    "def conv_conv_downsample(input_, filters):\n",
    "    c = conv_conv(input_, filters)\n",
    "    d = downsample(c, filters)\n",
    "    return c, d\n",
    "    \n",
    "def LF_Synthesis(input_=None):\n",
    "    with tf.name_scope('U-Net'):\n",
    "        with tf.name_scope('Encoder'):\n",
    "            c1, d1 = conv_conv_downsample(input_, 16)\n",
    "            c2, d2 = conv_conv_downsample(d1, 32)\n",
    "            c3, d3 = conv_conv_downsample(d2, 64)\n",
    "            c4, d4 = conv_conv_downsample(d3, 128)\n",
    "            c5, d5 = conv_conv_downsample(d4, 256)\n",
    "\n",
    "            feature = conv_conv(d5, 512)\n",
    "            \n",
    "        with tf.name_scope('Decoder'):\n",
    "            u5 = upsample_concat(feature, c5, 256)\n",
    "            c6 = conv_conv(u5, 256)\n",
    "            u6 = upsample_concat(c6, c4, 128)\n",
    "            c7 = conv_conv(u6, 128)\n",
    "            u7 = upsample_concat(c7, c3, 64)\n",
    "            c8 = conv_conv(u7, 64)\n",
    "            u8 = upsample_concat(c8, c2, 64)\n",
    "            c9 = conv_conv(u8, 64)\n",
    "            u9 = upsample_concat(c9, c1, 64)\n",
    "            c10 = conv_conv(u9, 64)\n",
    "            \n",
    "            flow_LF = flow_layer(c10)\n",